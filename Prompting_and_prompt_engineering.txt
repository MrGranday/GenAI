//* Prompting and prompt engineering*//

an LLM compute a distribution over the next word in the sequence which i will be illustrating with the probabilities under the visualized words 

I WROTE TO THE ZOO TO SEND ME A PET. THEY SENT ME A..............

words Probability (example)
line ....0.1
elephant....0.1
dog.....0.3
cat.... 0.2
panther...0.05


to exert some control over the LLM we can affect the probability over vocabulary in 2 ways 
// Prompting
// Training

//* Prompting   *//

the simplest way to affect the distribution over the vocabulary is to change the prompt
=> the text provided to an LLM  as input sometimes containing instructions and/or example
 
 the example that we have talked about above 
 ........ they sent me a little .......

 so in here the probability of the smaller animals goes up and the probability of the larger animals goes down



//* Prompt engineering*//

 the process of iteratively refining a prompt for the purpose of eliciting a particular style of response 
 in the prompt engineering you would br actually generating text form a model and seeing wither the generated text looks good
=> if i even have a smaller change that could be a white space that could the and have effect on the distribution over vocabulary words

their are few strategies ....


//*In-context learning *//
conditioning (prompting ) an LLM with instruction and or demonstrating of the task it is meant to complete 


//example
this is the GPT prompt example the purpose of this prompt is to translate the french language to english
/////this is the example of 3-shot prompting


|-------------------------------
|translate English to French     --------> task description 
|-------------------------------

|----------------------------------
|sea otter => louter de mer
|peppermint => menthe poviree    ---------> examples
|push girafe => girafe peluch
|----------------------------------

|---------------------------------
| cheese =>         -----------------------> prompt 
|---------------------------------


//example
//////this is the example of 2-shot prompting  
// NOTE:  the model will not actually perform the computation instead it will generate the probabilities over words 
// and most likely to follow the expression 1 + 8 :
Add 3+4: 7
Add 6+5: 11
Add 1+8: 





//*k-shot prompting *//
explicitly providing k examples of the intended task in the prompt



//* Advance Prompting strategies*//
// Chain-of-Thought-  Prompt the LLM to emit intermediate reasoning steps
(example):
Q: Roger has 5 tennis balls he buys 2 more cans of tennis balls each can has 3 tennis balls 
how many tennis balls does he have now?
A: Roger started with 5 balls 2 cans of 3 tennis balls each is 6 tennis balls 5+6=11 the answer is 11

/// this chain of thought can solve some of the hardest questions 

//*Least-to-most*//
prompt the LLM to decompose the problem and solve , easy-first
Start by solving the easiest parts of the problem first.
Then slowly move toward the harder parts.

//*Step-Back*//
// this introduce by Deepmind and it was used for enhancing the performance in chemistry and physics 